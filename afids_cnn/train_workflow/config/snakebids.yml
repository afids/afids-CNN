---
bids_dir: /path/to/bids_dir
output_dir: /path/to/output_dir

# enable printing debug statements during parsing -- disable if generating dag visualization
debug: false
derivatives: false  # will search in bids/derivatives if True; can also be path(s) to derivatives datasets

# list of analysis levels in the bids app
analysis_levels: &analysis_levels
  - participant
# mapping from analysis_level to set of target rules or files
targets_by_analysis_level:
  participant:
    - ''  # if '', then the first rule is run
# this configures the pybids grabber - create an entry for each type of input you want to grab
# indexed by name of input
#   dictionary for each input is passed directly to pybids get()
#    https://bids-standard.github.io/pybids/generated/bids.layout.BIDSLayout.html#bids.layout.BIDSLayout.get
pybids_inputs:
  T1w:
    filters:
      suffix: T1w
      extension: .nii.gz
      datatype: anat
      space: [MNI152NLin2009cAsym, False]
    wildcards: [subject, session, acquisition]
pybids_inputs_afids:
  afids:
    filters:
      suffix: afids
      extension: .fcsv
      space: [MNI152NLin2009cAsym, T1w, False]
      desc: [groundtruth, False]
      datatype: [anat, false]
    wildcards:
      - subject
      - session
      - description
# this configures the options to save the BIDSLayout
# by default, database is not saved (uncomment to save)
# NOTE: pybids_db_dir must be an absolute path
# pybids_db_dir: '/path/to/db_dir'  # Leave blank if you do not wish to use this
# pybids_db_reset: False  # Change this to true to update the database
# configuration for the command-line parameters to make available
# passed on the argparse add_argument()
parse_args:
#---  core BIDS-app options --- (do not modify below)
  bids_dir:
    help: The directory with the input dataset formatted according to the BIDS standard.
  output_dir:
    help: The directory where the output files should be stored. If you are running
      group level analysis this folder should be prepopulated with the results of
      the participant level analysis.
  analysis_level:
    help: Level of the analysis that will be performed.
    choices: *analysis_levels
  --participant_label:
    help: The label(s) of the participant(s) that should be analyzed. The label corresponds
      to sub-<participant_label> from the BIDS spec (so it does not include "sub-").
      If this parameter is not provided all subjects should be analyzed. Multiple
      participants can be specified with a space separated list.
    nargs: +
  --exclude_participant_label:
    help: The label(s) of the participant(s) that should be excluded. The label corresponds
      to sub-<participant_label> from the BIDS spec (so it does not include "sub-").
      If this parameter is not provided all subjects should be analyzed. Multiple
      participants can be specified with a space separated list.
    nargs: +
  --derivatives:
    help: 'Path(s) to a derivatives dataset, for folder(s) that contains multiple
      derivatives datasets (default: %(default)s) '
    default: false
    nargs: +

 # custom command-line parameters can then be added, these will get added to the config

  --afids_dir:
    help: 'Path to a BIDS dataset with AFIDs placements corresponding to bids_dir.'
    required: True
  --validation_bids_dir:
    help: 'Path to a validation BIDS dataset.'
  --validation_afids_dir:
    help: 'Path to a BIDS dataset with AFIDs placements corresponding to validation_bids_dir'
  --frequency:
    help: 'Augmentation frequency in voxels.'
    default: 1000
  --angle_stdev:
    help: 'Standard deviation of rotation angles applied in augmentation.'
    default: 30
  --num_augment:
    help: 'Number of augmentations to make.'
    default: 1
  --radius:
    help: 'Radius of patches to use.'
    default: 31
  --epochs:
    help: 'Number of training epochs to use.'
    default: 100
  --steps_per_epoch:
    help: 'Steps to use per training epoch.'
    default: 50
  --loss_fn:
    help: 'Tensorflow loss function to use.'
    default: 'mse'
  --metrics:
    help: 'List of tensorflow metrics to use.'
    nargs: '*'
    default: ['RootMeanSquaredError']
  --optimizer:
    help: 'Optimizer to use.'
    default: 'adam'
  --validation_steps: 
    help: 'Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.'
    default: 50
  --do_early_stopping:
    help: 'Use Early Stopping (patience = 100).'
    action: 'store_true'

#--- workflow specific configuration -- below is just an example:
num_channels: 2

containers:
  c3d: docker://khanlab/neuroglia-core:v1.5
